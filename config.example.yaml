# CloudOps Configuration Example
# Generated by: cloudops init
# Location: ~/.cloudops/config.yaml

# AI Provider Configuration
ai:
  # Provider: openai, anthropic, google, bedrock, deepseek, local, none
  provider: anthropic
  
  # Model ID (provider-specific)
  model: claude-3-5-sonnet-20241022
  
  # Reasoning mode: fast, balanced, deep
  reasoning: balanced
  
  # Credential configuration
  credentials:
    # Source: env, keychain, skip
    source: env
    # Environment variable name (when source=env)
    env_var: ANTHROPIC_API_KEY
  
  # Provider-specific configuration (optional)
  # For local models:
  # base_url: http://localhost:11434/v1
  # For bedrock:
  # region: us-east-1

# Legacy LLM config (maintained for backward compatibility)
llm:
  provider: anthropic
  model: claude-3-5-sonnet-20241022
  api_key_source: env:ANTHROPIC_API_KEY
  max_tokens: 8192
  temperature: 0.0

# Cloud provider configuration
cloud:
  # Primary cloud provider: aws, gcp, azure
  primary: aws
  
  # Enable real API calls (false = mock mode)
  use_real_apis: false

# Policy and authorization
policy:
  # Actions requiring approval
  require_approval_for:
    - write
    - delete
  
  # Actions that can auto-approve
  auto_approve:
    - read
  
  # Scope-specific policies
  scopes:
    prod: require_approval
    dev: auto_approve_reads
    staging: auto_approve_reads

# Example configurations for different providers:

# OpenAI:
# ai:
#   provider: openai
#   model: gpt-4o
#   reasoning: balanced
#   credentials:
#     source: env
#     env_var: OPENAI_API_KEY

# Google Gemini:
# ai:
#   provider: google
#   model: gemini-1.5-pro
#   reasoning: balanced
#   credentials:
#     source: env
#     env_var: GOOGLE_API_KEY

# AWS Bedrock:
# ai:
#   provider: bedrock
#   model: anthropic.claude-3-5-sonnet-20241022-v2:0
#   reasoning: balanced
#   region: us-east-1
#   credentials:
#     source: env
#     env_var: AWS_ACCESS_KEY_ID

# DeepSeek:
# ai:
#   provider: deepseek
#   model: deepseek-chat
#   reasoning: balanced
#   credentials:
#     source: env
#     env_var: DEEPSEEK_API_KEY

# Local (Ollama/LM Studio):
# ai:
#   provider: local
#   model: llama3
#   reasoning: balanced
#   base_url: http://localhost:11434/v1
#   credentials:
#     source: skip

# Planning-only (no LLM):
# ai:
#   provider: none
#   model: none
#   reasoning: balanced
#   credentials:
#     source: skip
